{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "전처리0+Bert 임베딩.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzahTtbQtTRd"
      },
      "source": [
        "# **결과**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pn6yzd2tYtc"
      },
      "source": [
        "## preprocssing0 + CNN_BiLSTM -> 제출 시: 0.50141 / - loss: 0.6147 - accuracy: 0.8757 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XomIEgORCR-d"
      },
      "source": [
        "# **데이터 전처리0 & 임베딩**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH09LfL1_xpS"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/dacon 기후기술분류 ai 데이터/train.csv\")\n",
        "labels = pd.read_csv(\"/content/drive/MyDrive/dacon 기후기술분류 ai 데이터/labels_mapping.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmoVsXKkScod"
      },
      "source": [
        "test = pd.read_csv(\"/content/drive/MyDrive/dacon 기후기술분류 ai 데이터/test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mRbRJlrH8M3"
      },
      "source": [
        "0. 우선, 제출년도, 사업명, 사업_부처명, 계속과제여부, 내역사업명, 과제명 컬럼을 합쳐 전처리를 시도해보려고 한다.(null값이 존재하지 않는 컬럼들)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZS_3n1WzIsk5"
      },
      "source": [
        "komoran의 형태소 추출 함수를 쓰려고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XOPC-OSJuEb"
      },
      "source": [
        "제출년도, 사업명, 사업_부처명, 계속과제여부, 내역사업명 컬럼 같은 경우는 거의 불용어 같은 것이 없으므로 불용어 제거, 토큰나이징에는 참여하지 않는다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sdEYULPIIuE"
      },
      "source": [
        "import re\n",
        "def preprocessing(text, komoran, remove_stopwords=False, stop_words=[]):\n",
        "  #년도 때문에 0-9 포함\n",
        "  #영어는 보통 그냥 단어(명사)인 경우로 명시되는 경우가 많아 그냥 영어는 전처리 안 하고 그대로 쓰도록 한다.\n",
        "  text=re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ0-9a-zA-Z]\",\"\", text)\n",
        "  if komoran=='okt':\n",
        "    word_text=komoran.morphs(text,stem=True)\n",
        "  else:\n",
        "    word_text=komoran.morphs(text)\n",
        "  if remove_stopwords:\n",
        "    word_review=[token for token in word_text if not token in stop_words]\n",
        "  return word_review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucblL1KVOpgI",
        "outputId": "2c45a8f8-83a0-4b67-fb7f-5d29b4b9eda5"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 57.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ58hKTWJJgJ"
      },
      "source": [
        "from konlpy.tag import Komoran\n",
        "from konlpy.tag import Okt\n",
        "#전처리 과정에서 모든 label에 공통적으로 쓰이는 단어도 포함\n",
        "stop_words=['를','활용한','은','에','는','이','가', '하','아','것','들','의','있','되','수','보','주','등','한','위한','및','통한','개발','이용한','기술','연구','활용한','기반','기반의','미치는','개발을','대한','의한','관한']\n",
        "komoran=Komoran()\n",
        "okt=Okt()\n",
        "train_text=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs1EmZ88M-u1",
        "outputId": "64e0c567-9e01-47dd-f9a9-d6142e11b623"
      },
      "source": [
        "import tqdm\n",
        "\n",
        "for i in tqdm.tqdm(range(len(train))):\n",
        "  try:\n",
        "    #제출년도+사업명+사업_부서명+계속과제여부+내역사업명+과제명\n",
        "    text = str(train.iloc[i,1])+\" \"+train.iloc[i,2]+\" \"+train.iloc[i,3]+\" \"+train.iloc[i,4]+\" \"+train.iloc[i,5]+\" \"+train.iloc[i,6]\n",
        "    train_text.append(preprocessing(text, komoran, remove_stopwords=True, stop_words=stop_words))\n",
        "  except:\n",
        "    train_text.append([])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 174304/174304 [04:41<00:00, 620.07it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTHfL3tgSqr3"
      },
      "source": [
        "test_text=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OacpVxuNRyj8",
        "outputId": "34e9a350-0d37-463d-936e-9dd19dd47488"
      },
      "source": [
        "for i in tqdm.tqdm(range(len(test))):\n",
        "  try:\n",
        "    text= str(test.iloc[i,1])+\" \"+test.iloc[i,2]+\" \"+test.iloc[i,3]+\" \"+test.iloc[i,4]+\" \"+test.iloc[i,5]+\" \"+test.iloc[i,6]\n",
        "    test_text.append(preprocessing(text, komoran, remove_stopwords=True, stop_words=stop_words))\n",
        "  except:\n",
        "    test_text.append([])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 43576/43576 [01:15<00:00, 580.84it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR8Pl3VUTPhC"
      },
      "source": [
        "# **임베딩**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27Rgz8CkS7-d",
        "outputId": "88164d4a-f7ad-4e3a-c8f6-f57d6294fe02"
      },
      "source": [
        "len(train_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "174304"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnFzP8wzTXxy",
        "outputId": "f509fe7f-6bfb-4f71-c89c-58125b2fc663"
      },
      "source": [
        "len(test_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43576"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnZfmnjZnv-j"
      },
      "source": [
        "**1. CountVectorizer를 통한 임베딩**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLaqhGm6TY7_"
      },
      "source": [
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# #tokenizer 인자에는 list를 받아서 그대로 내보내는 함수를 넣어줍니다. 또한 소문자화를 하지 않도록 설정해야 에러가 나지 않습니다.\n",
        "# vectorizer = CountVectorizer(tokenizer = lambda x: x, lowercase=False)\n",
        "# train_features=vectorizer.fit_transform(train_text)\n",
        "# test_features=vectorizer.transform(test_text)\n",
        "# #test데이터에 fit_transform을 할 경우 data leakage에 해당합니다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntOHmK-oUJ1S"
      },
      "source": [
        "# def tokenize_text(text):\n",
        "#     '''토큰화 하여 인덱스를 돌려주는 함수\n",
        "\n",
        "#     Args:\n",
        "#         text: 문서들을 원소로 하는 리스트\n",
        "\n",
        "#     Return:\n",
        "#         토큰화하고 그것들을 id값으로 반환하는 리스트\n",
        "#     '''\n",
        "#     return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd1jT2y7UOcB"
      },
      "source": [
        "# tokenized_text = [tokenize_text(text) for text in train_features ]\n",
        "# #tokenized_test = [tokenize_text(test_txt) for test_txt in test_features]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz-4D66QzlUK"
      },
      "source": [
        "# train['train_text']=0\n",
        "# train['train_text']=str(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjM-e44pzdI4"
      },
      "source": [
        "# train['train_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTsiqsKL0CM7"
      },
      "source": [
        "# test['test_text']=0\n",
        "# for i in range(len(test_text)):\n",
        "#   test.loc[i,'test_text']=str(test_text[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_ci_axzoAm-"
      },
      "source": [
        "**2. Bert의 토크나이저를 불러와 토크나이즈 하기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeGO3xnjoNuc",
        "outputId": "179a6fe8-4560-47b7-e677-54c598e9d437"
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n",
            "\u001b[?25l\r\u001b[K     |████████                        | 10 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20 kB 34.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30 kB 31.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 41 kB 134 kB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading params-flow-0.8.2.tar.gz (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30536 sha256=0c1738080c35e474d17aca8c3054b4f750f43c0d48d5b67734786f051da8f911\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/b6/e5/8c76ec779f54bc5c2f1b57d2200bb9c77616da83873e8acb53\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19471 sha256=038e95c4c26295f4710fa999534b335ba3ef1e76746687da67a65a915a0f27b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7910 sha256=836da5f62c27bb766b5379bb40ce3ebf810f60f0d5ead991c54c1409863c9a06\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/11/67/33cc51bbee127cb8fb2ba549cd29109b2f22da43ddf9969716\n",
            "Successfully built bert-for-tf2 params-flow py-params\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqvp5CzZoE9O"
      },
      "source": [
        "# Bert의 토크나이저를 불러와 이를 이용하여 토크나이즈할 수 있도록 함\n",
        "import bert\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)\n",
        "vocab = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertTokenizer(vocab, to_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T91kRyDnoreU"
      },
      "source": [
        "def tokenize_text(text,tokenzier):\n",
        "  all_tokens=[]\n",
        "  '''토큰화 하여 인덱스를 돌려주는 함수\n",
        "\n",
        "  Args:\n",
        "    text: 문서들을 원소로 하는 리스트\n",
        "\n",
        "  Return:\n",
        "      토큰화하고 그것들을 id값으로 반환하는 리스트\n",
        "  '''\n",
        "  for texts in text:\n",
        "    tokens_a = tokenizer.tokenize(str(texts))\n",
        "    one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])\n",
        "    all_tokens.append(one_token)\n",
        "  return np.array(all_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFjUzf0cosgn",
        "outputId": "cd66f88f-7509-41b2-8772-4083468357be"
      },
      "source": [
        "#tokenized_train = [tokenize_text(text) for text in train_text]\n",
        "tokenized_train = tokenize_text(train_text,tokenizer)\n",
        "#tokenized_test = [tokenize_text(test_txt) for test_txt in test_text]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipLSk79w1QOA",
        "outputId": "c9de7b81-18c6-493c-a0f7-04a8684495ec"
      },
      "source": [
        "tokenized_test = tokenize_text(test_text,tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYaDZNHT2Dh-"
      },
      "source": [
        "def get_max_length(df):\n",
        "    '''단어 기준으로 문서들 중 가장 긴 것의 길이를 반환하는 함수\n",
        "    \n",
        "    padding을 하기 위해 최대값을 찾을 수 있도록 개별 작성한 함수\n",
        "\n",
        "    Args:\n",
        "        df: 원본 데이터프레임을 입력받음\n",
        "\n",
        "    Return:\n",
        "        max_length: 문서들 중 최대값을 반환\n",
        "    '''\n",
        "    max_length = 0\n",
        "    for row in df:\n",
        "        if len(str(row).split(\" \")) > max_length:\n",
        "            max_length = len(str(row).split(\" \"))\n",
        "    return max_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdpuXLrB2A27"
      },
      "source": [
        "MAX_SEQ_LEN = get_max_length(train_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zClrwvDT2wUt"
      },
      "source": [
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "train_x = preprocessing.sequence.pad_sequences(tokenized_train, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "test_x = preprocessing.sequence.pad_sequences(tokenized_test, maxlen=MAX_SEQ_LEN, padding='post' )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}